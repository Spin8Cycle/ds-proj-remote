{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "* Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
    "* Because they are so fast and have so few tunable parameters, they end up being useful as a quick-and-dirty baseline for a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Bayesian Classification:\n",
    "\n",
    "* Naive Bayes classifiers are built on Bayesian classification methods.\n",
    "* Bayes's Theorem:\n",
    "    * Equation describing the relationship of conditional probabilities of statistical quantities.\n",
    "    * In Bayesian classification, we're interested in finding the probability of a LABEL L given some observed features, which we can write as : \n",
    "        * $P(L\\ |\\ features)$\n",
    "    * $P(L\\ |\\ features)=\\frac{P(L\\ |\\ features)P(L)}{P(features)}$\n",
    "\n",
    "* If we are trying to decide between two labels ($L_1, L_2$), then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "    * $\\frac{P(L_1\\ |\\ features)} {P(L_2\\ |\\ features)} = \\frac{P(features\\ |\\ L_1)P(L_1)}{P(features\\ |\\ L_2)P(L_2)}$\n",
    "\n",
    "* All we need now is some model by which we can compute $P(features\\ |\\ L_i)$ for each label.\n",
    "    * Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
    "    * Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
    "    * The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
    "    * This is where the \"naive\" in \"naive Bayes\" comes in:\n",
    "        * If we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.\n",
    "        * Different types of naive Bayes classifiers rest on different naive assumptions about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
