{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests\n",
    "\n",
    "* If you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor.\n",
    "\n",
    "* A group of predictors is called an ensemble; thus, this technique is called ensemble learning, and an ensemble learning algorithm is called an ensemble method.\n",
    "    * Example: You can train a group of decision tree classifiers, each on a different random subset of the training set. You can then obtain the predictions of all the individual trees, and the class that gets the most votes is the ensemble’s prediction. Such an ensemble of decision trees is called a random forest, and despite its simplicity, this is one of the most powerful machine learning algorithms available today.\n",
    "\n",
    "\n",
    "# A. Voting Classifiers:\n",
    "* Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a logistic regression classifier, an SVM classifier, a random forest classifier, a k-nearest neighbors classifier, and perhaps a few more: <br><br>\n",
    "    &emsp; &emsp; ![Alt text](image-10.png)\n",
    "    <br><br>\n",
    "    \n",
    "* **Hard Voting** classifier:\n",
    "    * Majority-vote classifier , after aggregating the predictions of each classifier, the class that gets the most votes is the ensembles prediction.\n",
    "    <br><br>\n",
    "    &emsp; &emsp; ![Alt text](image-11.png)\n",
    "\n",
    "* Even if each classifier is a weak learner (meaning it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners in the ensemble and they are sufficiently diverse. \n",
    "    * Law of Large Numbers:  The law of large numbers states that the the average of results from a large number of experiment trials:   \n",
    "        * should be close to the expected value \n",
    "        * will move closer to the expected value as more trials are performed\n",
    "\n",
    "    * Suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing.). If you predict the majority voted class, you can hope for up to 75% accuracy. However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.\n",
    "\n",
    "    * Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
